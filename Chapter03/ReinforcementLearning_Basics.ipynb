{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3158f7",
   "metadata": {},
   "source": [
    "# Kapitel 3: Grundlagen zu Reinforcement Learning (RL)\n",
    "\n",
    "In diesem Kapitel werden die grundlegenden Konzepte des Reinforcement Learnings (RL) vorgestellt. Dazu gehören Markov-Prozesse, Markov-Entscheidungsprozesse (MDPs), Wertfunktionen und die Bellman-Gleichung. Diese Konzepte bilden die Grundlage für das Verständnis und die Implementierung von RL-Algorithmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c351a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a309b",
   "metadata": {},
   "source": [
    "## Markov-Prozess (MDP) - Zustände, Aktionen und Belohnungen\n",
    "\n",
    "Defintion der Zustände, Aktionen und Belohnungen in einem einfachen MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "311fb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATES = [\"a\", \"b\"]\n",
    "ACTIONS = [\"a\", \"b\"]\n",
    "REWARDS = {\"a\": {\"a\": 0, \"b\": 7}, \"b\": {\"a\": -5, \"b\": 0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a6f123",
   "metadata": {},
   "source": [
    "Initialisierung der Anfangswerte für Zustand, Belohnung und Gesamtbelohnung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0d57f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start-State: a Start-Reward: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = \"a\"\n",
    "reward = 0\n",
    "total_reward = 0\n",
    "\n",
    "# Ausgabe des Startzustands und der Startbelohnung\n",
    "print(f\"Start-State: {state} Start-Reward: {reward}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392030b7",
   "metadata": {},
   "source": [
    "Simulation von 10 Iterationen, in denen der Benutzer Aktionen auswählt und die entsprechenden Belohnungen und Gesamtbelohnungen berechnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d01da07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: a - Iteration: 1\n",
      "New State: a Reward: 0 Total-Reward: 0\n",
      "State: a - Iteration: 2\n",
      "New State: a Reward: 0 Total-Reward: 0\n",
      "State: a - Iteration: 3\n",
      "New State: b Reward: 7 Total-Reward: 7\n",
      "State: b - Iteration: 4\n",
      "New State: b Reward: 0 Total-Reward: 7\n",
      "State: b - Iteration: 5\n",
      "New State: a Reward: -5 Total-Reward: 2\n",
      "State: a - Iteration: 6\n",
      "New State: b Reward: 7 Total-Reward: 9\n",
      "State: b - Iteration: 7\n",
      "New State: a Reward: -5 Total-Reward: 4\n",
      "State: a - Iteration: 8\n",
      "New State: b Reward: 7 Total-Reward: 11\n",
      "State: b - Iteration: 9\n",
      "New State: a Reward: -5 Total-Reward: 6\n",
      "State: a - Iteration: 10\n",
      "New State: b Reward: 7 Total-Reward: 13\n"
     ]
    }
   ],
   "source": [
    "# Simuliere 10 Iterationen\n",
    "for i in range(1, 11):\n",
    "    print(f\"State: {state} - Iteration: {i}\")\n",
    "    action = input(\"Action: \")\n",
    "    if action in ACTIONS:\n",
    "        reward = REWARDS[state][action]\n",
    "        total_reward += reward\n",
    "        state = action\n",
    "        print(\n",
    "            f\"New State: {state} \"\n",
    "            f\"Reward: {reward} \"\n",
    "            f\"Total-Reward: {total_reward}\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e296a",
   "metadata": {},
   "source": [
    "Ein schlauer Agent wird in der Lage sein, eine Strategie zu entwickeln, um die Gesamtbelohnung zu maximieren.\n",
    "\n",
    "Strategie: \n",
    "\n",
    "Wechsel zwischen den Zuständen \"a\" und \"b\", um die Belohnung zu maximieren.\n",
    "\n",
    "a -> b -> a -> b -> a -> b -> a -> b -> a -> b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff090ed",
   "metadata": {},
   "source": [
    "## Markov reward process (MRP) - Makrovorgang mit Belohnung\n",
    "\n",
    "Hinzunahme von Wahrscheinlichkeiten für Zustandsübergänge und Berechnung der erwarteten Belohnung.\n",
    "\n",
    "$ G_t = R_{t+1} + γR_{t+2} + γ^2R_{t+3} + ... = Σ (γ^k * R_{t+k+1}) $\n",
    "\n",
    "\n",
    "Definition der Zustände, Aktionen, Belohnungen und Übergangswahrscheinlichkeiten in einem MRP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993b468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zustände\n",
    "STATES = [\"a\", \"b\"]\n",
    "# Aktionen\n",
    "ACTIONS = [\"a\", \"b\"]\n",
    "# Belohnungen für Aktionen in Zuständen\n",
    "REWARDS = {\"a\": {\"a\": 0, \"b\": 7}, \"b\": {\"a\": -5, \"b\": 0}}\n",
    "# Wahrscheinlichkeiten für Zustandsübergänge (entspricht der Policy)\n",
    "TRANSITIONS = {\"a\": {\"a\": 0.1, \"b\": 0.9}, \"b\": {\"a\": 0.9, \"b\": 0.1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050a460",
   "metadata": {},
   "source": [
    "Initialisierung der Anfangswerte für Zustand, Belohnung und Gesamtbelohnung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc222235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start-State: a Start-Reward: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = \"a\"\n",
    "all_states = state\n",
    "reward = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(f\"Start-State: {state} Start-Reward: {reward}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88de5bf",
   "metadata": {},
   "source": [
    "Markov reward process (MRP) - Makrovorgang mit Belohnung (10 Iterationen=Episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce1d36c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: a - Iteration: 1\n",
      "New State: b Reward: 7 Total-Reward: 15\n",
      "State: b - Iteration: 2\n",
      "New State: a Reward: -5 Total-Reward: 10\n",
      "State: a - Iteration: 3\n",
      "New State: b Reward: 7 Total-Reward: 17\n",
      "State: b - Iteration: 4\n",
      "New State: b Reward: 0 Total-Reward: 17\n",
      "State: b - Iteration: 5\n",
      "New State: a Reward: -5 Total-Reward: 12\n",
      "State: a - Iteration: 6\n",
      "New State: b Reward: 7 Total-Reward: 19\n",
      "State: b - Iteration: 7\n",
      "New State: a Reward: -5 Total-Reward: 14\n",
      "State: a - Iteration: 8\n",
      "New State: b Reward: 7 Total-Reward: 21\n",
      "State: b - Iteration: 9\n",
      "New State: a Reward: -5 Total-Reward: 16\n",
      "State: a - Iteration: 10\n",
      "New State: b Reward: 7 Total-Reward: 23\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "    print(f\"State: {state} - Iteration: {i}\")\n",
    "    action = input(\"Action: \")\n",
    "    if action in ACTIONS:\n",
    "        t = np.random.choice(\n",
    "            len(STATES),\n",
    "            p=list(TRANSITIONS[state].values()),\n",
    "        )\n",
    "        transition = STATES[t]\n",
    "        reward = REWARDS[state][transition]\n",
    "        total_reward += reward\n",
    "        state = transition\n",
    "        all_states += \" -> \" + state\n",
    "        print(\n",
    "            f\"New State: {state} \"\n",
    "            f\"Reward: {reward} \"\n",
    "            f\"Total-Reward: {total_reward}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a341f54",
   "metadata": {},
   "source": [
    "## Markov Decision process (MDP) - Zustände, Aktionen, Belohnungen und Übergangswahrscheinlichkeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2898c8e0",
   "metadata": {},
   "source": [
    "Formel zu Berechnung der diskontierten Belohnung bei Bekanntheit der Rewards und des Discount-Faktors (gamma).\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c60eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_reward(rewards: list, gamma: float) -> float:\n",
    "    result = 0.0\n",
    "    for t in range(len(rewards)):\n",
    "        result += gamma**t * rewards[t]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c687dd5",
   "metadata": {},
   "source": [
    "Wie wirkt sich der Discount-Faktor (gamma) auf die Berechnung der diskontierten Belohnung aus?\n",
    "\n",
    "Die Rewards sind bekannt und werden mit zunehmender Zeit weniger wertvoll, je kleiner gamma ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1961de66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.7\n",
      "rewards: [1, 1, 1, -1, -1, 1, -1]\n",
      "discounted_reward_value: 1.657321\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.7  # [0, 1]\n",
    "rewards = [1, 1, 1, -1, -1, 1, -1]\n",
    "\n",
    "discounted_reward_value = discounted_reward(rewards, gamma)\n",
    "print(f\"gamma: {gamma}\")\n",
    "print(f\"rewards: {rewards}\")\n",
    "print(f\"discounted_reward_value: {discounted_reward_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
